{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of roxana - mpi para Python.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kVWqN3EhJFbJ",
        "LJ_ThE3-KthZ",
        "Vww7AO93S8QD",
        "49L8NojO6axL",
        "XxKI85d8LLll",
        "UKgBEDizMkRM",
        "lFx0VyZzN1Aw",
        "eJQfgyqlOTCr",
        "o6bvSqgMPeyt",
        "57wzDFu2xxvW",
        "n3eZ1sN1yVsK",
        "tNe1kQLpyqh2",
        "FcckAX8G0Pd-",
        "ocfu_ZYw3QCz",
        "0gcBP4q93eCj",
        "IGWlHIPh4t08",
        "cwiw83d44_gR",
        "PrwM90WO6jKv",
        "YRLLMm9j7iOA",
        "JnO-S8V37tpD",
        "P74yRNds9SBx",
        "gL2CQgIo-xTd",
        "1g5HzFU1--zY",
        "H9EYqJldBRS-",
        "tIw9YI6GB5go",
        "mPVwi5W4DpwJ",
        "-eCGJi2hEOC1",
        "qKhQXI7oFRMf",
        "t65YHXekHbBK"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParalelaUnsaac/G4-2020-1/blob/main/Copy_of_roxana_mpi_para_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXMYxZtKyP0I"
      },
      "source": [
        "# Distributed Parallel Programming Patterns using mpi4py\n",
        "Libby Shoop, Macalester College\n",
        "\n",
        "¡Bienvenido!\n",
        "\n",
        "Este libro contiene algunos ejemplos que ilustran los conceptos fundamentales básicos de la computación distribuida utilizando código Python. El tipo de computación que ilustran estos ejemplos se llama * paso de mensajes *. El paso de mensajes es una forma de programación que se basa en procesos que se comunican entre sí para coordinar su trabajo. El paso de mensajes se puede utilizar en una sola computadora multinúcleo o con un grupo de computadoras.\n",
        "\n",
        "\n",
        "### Patrones de software\n",
        "\n",
        "Los patrones en el software son implementaciones comunes que los profesionales han utilizado una y otra vez para realizar tareas. A medida que los profesionales los usan repetidamente, la comunidad comienza a darles nombres y catalogarlos, convirtiéndolos a menudo en funciones de biblioteca reutilizables. Los ejemplos que verá en este libro se basan en patrones documentados que se han utilizado para resolver diferentes problemas mediante el paso de mensajes entre procesos. El paso de mensajes es una forma de computación distribuida que usa procesos, que se pueden usar en grupos de computadoras o máquinas multinúcleo.\n",
        "\n",
        "En muchos de estos ejemplos, el nombre del patrón es parte del nombre del archivo de código Python. También verá que, a menudo, las funciones de la biblioteca MPI también toman el nombre del patrón, y la implementación de esas funciones contiene el patrón que los profesionales utilizan con frecuencia. Estos ejemplos de código de patrón que le mostramos aquí, denominados patternlets, se basan en el trabajo original de Joel Adams:\n",
        "\n",
        "Adams, Joel C. \"Patternlets: una herramienta de enseñanza para presentar a los estudiantes los patrones de diseño paralelos\". Taller del Simposio Internacional de Procesamiento Distribuido y Paralelo del IEEE 2015. IEEE, 2015.\n",
        "\n",
        "Para ejecutar estos ejemplos, primero necesitará instalar la biblioteca mpi4py ejecutando este código (esto generalmente llevará un tiempo instalarlo la primera vez):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVgDVtdkxrgc",
        "outputId": "101b44c2-6586-402b-d564-01534a4ed138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "! pip install mpi4py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mpi4py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/8f/bbd8de5ba566dd77e408d8136e2bab7fdf2b97ce06cab830ba8b50a2f588/mpi4py-3.0.3.tar.gz (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 3.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.0.3-cp36-cp36m-linux_x86_64.whl size=2074496 sha256=45dcf1c92f032d7cccbaaae87b0ccc6bd6b08aba1a5577154fcf31abb79f52ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/e0/86/2b713dd512199096012ceca61429e12b960888de59818871d6\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqLzdrkgv2rw"
      },
      "source": [
        "![Important symbol](https://drive.google.com/uc?id=1AWRLAqeaqi7SG7PHyOVywZRuMDK9Z2_s)**Important:** \n",
        "Importante: tendrá que volver a ejecutar esta celda después de desconectarse durante bastante tiempo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uryzb1Wy-hlp"
      },
      "source": [
        "# Patrones de estructura del programa\n",
        "\n",
        "> Bloque con sangría\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdWhWJMLzUIm"
      },
      "source": [
        "## Single Program, Multiple Data SPMD\n",
        "\n",
        "1.   Elemento de lista\n",
        "2.   Elemento de lista\n",
        "\n",
        "\n",
        "\n",
        "This code forms the basis of all of the other examples that follow. It is the fundamental way we structure parallel programs today.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djhcbZEkzIAB",
        "outputId": "e05009ba-03e5-4378-a74f-20699da49d34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 00spmd.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    print(\"Saludos desde el proceso {} de {} en {}\"\\\n",
        "    .format(id, numProcesses, myHostName))\n",
        "\n",
        "########## Run the main function\n",
        "main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing 00spmd.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqOAtb4G4-e2",
        "outputId": "452042c9-237d-49f5-a167-585c9958c993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "! mpirun --allow-run-as-root -np 4 python 00spmd.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saludos desde el proceso 0 de 4 en 4afe557c9fa0\n",
            "Saludos desde el proceso 3 de 4 en 4afe557c9fa0\n",
            "Saludos desde el proceso 2 de 4 en 4afe557c9fa0\n",
            "Saludos desde el proceso 1 de 4 en 4afe557c9fa0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR2tfQ8v8RVa"
      },
      "source": [
        "The fundamental idea of message passing programs can be illustrated like this:\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1wpQaFiaubIcQBV9Lw_jwOU0y2-K-EChW)\n",
        "\n",
        "Cada proceso se configura dentro de una red de comunicación para poder comunicarse con todos los demás procesos a través de enlaces de comunicación. Cada proceso está configurado para tener su propio número, o id, que comienza en 0.\n",
        "\n",
        "Nota: Cada proceso tiene sus propias copias de las 4 variables de datos anteriores. Entonces, aunque hay un solo programa, se ejecuta varias veces en procesos separados, cada uno con sus propios valores de datos. Esta es la razón del nombre del patrón que representa este código: programa único, datos múltiples. La línea de impresión al final de main () representa las múltiples salidas de datos diferentes producidas por cada proceso.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSEhP3vv-_yX"
      },
      "source": [
        "## Master-Worker\n",
        "Este también es un patrón muy común que se usa en programación paralela y distribuida. Aquí está el pequeño código ilustrativo de muestra. Revísalo y responde esto: ¿Qué es diferente entre este ejemplo y el anterior?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlF-_TYc_uKu",
        "outputId": "6141f0af-5a26-4030-d3af-a1429d02fd21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 01masterWorker.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    if id == 0:\n",
        "        print(\"Saludos del maestro, {} de {} en {}\"\\\n",
        "        .format(id, numProcesses, myHostName))\n",
        "    else:\n",
        "        print(\"Saludos del trabajador, {} de {} en {}\"\\\n",
        "        .format(id, numProcesses, myHostName))\n",
        "\n",
        "########## Run the main function\n",
        "main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing 01masterWorker.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ0rjAxS_9xK"
      },
      "source": [
        "La respuesta a la pregunta anterior ilustra lo que podemos hacer con este patrón: basándonos en la identificación del proceso, podemos hacer que un proceso lleve a cabo algo diferente a los demás. Este concepto se utiliza mucho como un medio para coordinar actividades, donde un proceso, a menudo llamado maestro, tiene la responsabilidad de entregar el trabajo y realizar un seguimiento de los resultados. Veremos esto en ejemplos posteriores.\n",
        "\n",
        "**Note:**\n",
        "Por convención, el proceso de coordinación maestro suele ser el proceso número 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8HeRMx-APS2",
        "outputId": "50527473-2870-4cc9-b324-74d23cc36fbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "! mpirun --allow-run-as-root -np 6 python 01masterWorker.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saludos del trabajador, 1 de 6 en 4ea9d432a738\n",
            "Saludos del trabajador, 4 de 6 en 4ea9d432a738\n",
            "Saludos del maestro, 0 de 6 en 4ea9d432a738\n",
            "Saludos del trabajador, 2 de 6 en 4ea9d432a738\n",
            "Saludos del trabajador, 3 de 6 en 4ea9d432a738\n",
            "Saludos del trabajador, 5 de 6 en 4ea9d432a738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9paZLu4Xt4Jm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Eyj6sa7GoXu"
      },
      "source": [
        "### Exercises:\n",
        "\n",
        "- Vuelva a ejecutar, utilizando un número variable de procesos del 1 al 8 (es decir, varíe el argumento después de -np).\n",
        "\n",
        "- Explique qué permanece igual y qué cambia a medida que cambia el número de procesos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1uIEnUS-JGG"
      },
      "source": [
        "# Descomposición usando paralelismo para patrones de bucles\n",
        "\n",
        "La forma más común de completar una tarea repetida en cualquier lenguaje de programa es un bucle. Usamos bucles porque queremos hacer un cierto número de tareas, muy a menudo porque queremos trabajar en un conjunto de elementos de datos que se encuentran en una lista o una matriz, o alguna otra estructura de datos. Si el trabajo a realizar en cada ciclo es independiente de las iteraciones anteriores, podemos usar procesos separados para hacer partes del ciclo de forma independiente. Este patrón de estructura de programa se denomina patrón de bucle para paralelizar, que es una estrategia de implementación para la descomposición del trabajo a realizar en partes más pequeñas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVWqN3EhJFbJ"
      },
      "source": [
        "## Parallel Loop Split into Equal Sized Chunks -> G1\n",
        "\n",
        "In the code below, notice the use of the variable called `REPS`. This is designed to be the total amount or work, or repetitions, that the for loop is accomplishing. This particular code is designed so that if those repetitions do not divide equally by the number of processes, then the program will stop with a warning message printed by the master process.\n",
        "\n",
        "Remember that because this is still also a SPMD program, all processes execute the code in the part of the if statement that evaluates to True. Each process has its own id, and we can determine how many processes there are, so we can choose where in the overall number of REPs of the loop each process will execute.\n",
        "\n",
        "En el código siguiente, observe el uso de la variable llamada \"REPS\". Esto está diseñado para ser la cantidad total de trabajo, o repeticiones, que el ciclo for está logrando. Este código en particular está diseñado para que si esas repeticiones no se dividen equitativamente por el número de procesos, el programa se detendrá con un mensaje de advertencia impreso por el proceso maestro.\n",
        "\n",
        "Recuerde que debido a que también es un programa SPMD, todos los procesos ejecutan el código en la parte de la instrucción if que se evalúa como Verdadero. Cada proceso tiene su propia identificación, y podemos determinar cuántos procesos hay, por lo que podemos elegir en qué parte del número total de REP del ciclo se ejecutará cada proceso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OpxV7pnJ_u2",
        "outputId": "24ff1f33-bf00-4a55-dafe-eae6b4d6af38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 02parallelLoopEqualChunks.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    REPS = 8\n",
        "\n",
        "    if ((REPS % numProcesses) == 0 and numProcesses <= REPS):\n",
        "        # How much of the loop should a process work on?\n",
        "        chunkSize = int(REPS / numProcesses) #5\n",
        "        start = id * chunkSize # 0 * 5 = 0 | 1 * 5 = 5\n",
        "        stop = start + chunkSize # 0 + 5 = 5 | 5 + 5 = 10\n",
        "\n",
        "        # do the work within the range set aside for this process\n",
        "        for i in range(start, stop): # 0 a 4\n",
        "            print(\"On {}: Process {} is performing iteration {}\"\\\n",
        "            .format(myHostName, id, i)) # dsa3123, 0, 0 | dsa3123, 0, 1 | dsa3123, 0, 2 | dsa3123, 0, 3 |dsa3123, 0, 5\n",
        "\n",
        "    else:\n",
        "        # cannot break into equal chunks; one process reports the error\n",
        "        if id == 0 :\n",
        "            print(\"Please run with number of processes divisible by \\\n",
        "and less than or equal to {}.\".format(REPS))\n",
        "\n",
        "########## Run the main function\n",
        "main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing 02parallelLoopEqualChunks.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBshPRYXLLhJ",
        "outputId": "dd09a69e-3d26-4aa5-d16b-455689cba5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "source": [
        "! mpirun --allow-run-as-root -np 4 python 02parallelLoopEqualChunks.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On 4a2861f4dd33: Process 0 is performing iteration 0\n",
            "On 4a2861f4dd33: Process 0 is performing iteration 1\n",
            "On 4a2861f4dd33: Process 2 is performing iteration 4\n",
            "On 4a2861f4dd33: Process 2 is performing iteration 5\n",
            "On 4a2861f4dd33: Process 3 is performing iteration 6\n",
            "On 4a2861f4dd33: Process 3 is performing iteration 7\n",
            "On 4a2861f4dd33: Process 1 is performing iteration 2\n",
            "On 4a2861f4dd33: Process 1 is performing iteration 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ_ThE3-KthZ"
      },
      "source": [
        "### Ejercicios\n",
        "\n",
        "- Run, using these numbers of processes, N: 1, 2, 4, and 8 (i.e., vary the  argument to -np).\n",
        "- Change REPS to 16 in the code and rerun it. Then rerun with mpirun, varying N again.\n",
        "- Explain how this pattern divides the iterations of the loop among the processes.\n",
        "\n",
        "Which of the following is the correct assignment of loop iterations to processes for this code, when REPS is 8 and numProcesses is 4?\n",
        "\n",
        "Ejecute, utilizando estos números de procesos, N: 1, 2, 4 y 8 (es decir, varíe el argumento a -np).\n",
        "- Cambie REPS a 16 en el código y vuelva a ejecutarlo. Luego, vuelva a ejecutar con mpirun, variando N nuevamente.\n",
        "- Explica cómo este patrón divide las iteraciones del bucle entre los procesos.\n",
        "\n",
        "¿Cuál de las siguientes es la asignación correcta de iteraciones de bucle a procesos para este código, cuando REPS es 8 y numProcesses es 4?\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1eUsjxYdWXWqThO_rdLO91HaLqBLAAh_S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_okom_QiQeHe"
      },
      "source": [
        "## Parallel for Loop Program Structure: chunks of 1\n",
        "\n",
        "In the code below, we again use the variable called `REPS` for the total amount or work, or repetitions, that the for loop is accomplishing. This particular code is designed so that the number of repetitions should be more than or equal to the number of processes requested.\n",
        ".. note:: Typically in real problems, the number of repetitions is much higher than the number of processes. We keep it small here to illustrate what is happening.\n",
        "\n",
        "Like the last example all processes execute the code in the part of the if statement that evaluates to True. Note that in the for loop in this case we simply have process whose id is 0 start at iteration 0, then skip to 0 + numProcesses for its next iteration, and so on. Similarly, process 1 starts at iteration 1, skipping next to 1+ numProcesses, and continuing until REPs is reached. Each process performs similar single 'slices' or 'chunks of size 1' of the whole loop.\n",
        "\n",
        "En el siguiente código, nuevamente usamos la variable llamada REPS para la cantidad total de trabajo, o repeticiones, que el ciclo for está logrando. Este código en particular está diseñado para que el número de repeticiones sea mayor o igual al número de procesos solicitados. .. nota :: Normalmente, en problemas reales, el número de repeticiones es mucho mayor que el número de procesos. Lo mantenemos pequeño aquí para ilustrar lo que está sucediendo.\n",
        "\n",
        "Como en el último ejemplo, todos los procesos ejecutan el código en la parte de la instrucción if que se evalúa como Verdadero. Tenga en cuenta que en el bucle for en este caso simplemente tenemos un proceso cuyo id es 0, comienza en la iteración 0, luego salta a 0 + numProcesses para su próxima iteración, y así sucesivamente. De manera similar, el proceso 1 comienza en la iteración 1, saltando junto a 1+ numProcesses y continuando hasta que se alcanzan los REP. Cada proceso realiza 'rebanadas' individuales similares o 'trozos de tamaño 1' de todo el ciclo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15B2xax5RXEY",
        "outputId": "b8533cb0-e4a7-4b6f-c4ae-184b8d0abc56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 03parallelLoopChunksOf1.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    REPS = 8\n",
        "\n",
        "    if (numProcesses <= REPS): #4<8\n",
        "\n",
        "        for i in range(id, REPS, numProcesses): # i (0,8,4) | (1,8,4)\n",
        "            print(\"On {}: Process {} is performing iteration {}\"\\\n",
        "            .format(myHostName, id, i)) #nrsdy656, 0, 0 | nrsdy656, 0, 4 |ssfs, 1, 1 | ssfs, 1, 5|\n",
        "\n",
        "    else:\n",
        "        # can't have more processes than work; one process reports the error\n",
        "        if id == 0 :\n",
        "            print(\"Please run with number of processes less than \\\n",
        "or equal to {}.\".format(REPS))\n",
        "\n",
        "########## Run the main function\n",
        "main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting 03parallelLoopChunksOf1.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0ewx6nLSLgV",
        "outputId": "4eea38a7-f879-4555-ecf6-a7a4b9e5b1b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "source": [
        "! mpirun --allow-run-as-root -np 4 python 03parallelLoopChunksOf1.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On 4a2861f4dd33: Process 1 is performing iteration 1\n",
            "On 4a2861f4dd33: Process 2 is performing iteration 2\n",
            "On 4a2861f4dd33: Process 2 is performing iteration 6\n",
            "On 4a2861f4dd33: Process 0 is performing iteration 0\n",
            "On 4a2861f4dd33: Process 1 is performing iteration 5\n",
            "On 4a2861f4dd33: Process 3 is performing iteration 3\n",
            "On 4a2861f4dd33: Process 3 is performing iteration 7\n",
            "On 4a2861f4dd33: Process 0 is performing iteration 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz1gqhjrWhVo"
      },
      "source": [
        "Para determinar el trabajo que va a realizar cada proceso, se tiene:\n",
        "\n",
        "1.  NroIteracionesPrimerosProcesos = REPS DIV NumProcesses + 1 | Llamamos PrimerosProcesos a los que van de 0 a (REPS - (REPS DIV NumProcesses) * NumProcesses - 1).\n",
        "2.  NroIteracionesUltimosProcesos = REPS DIV NumProcesses | Llamamos UltimosProcesos a los que van de (REPS - (REPS DIV NumProcesses) * NumProcesses) a (NumProcesses - 1).\n",
        "\n",
        "Por ejemplo: REPS = 8 & NumProcesses = 3\n",
        "\n",
        "Entonces: \n",
        "\n",
        "1.   NroIteracionesPrimerosProcesos = 8 DIV 3 + 1 = 3 || PrimerosProcesos van de 0 a (8 - (8 DIV 3) * 3 - 1 = 1).\n",
        "\n",
        "    Es decir, los primeros procesos van de 0 a 1 y cada uno hará 3 iteraciones\n",
        "\n",
        "     \n",
        "2.   NroIteracionesUltimosProcesos = 8 DIV 3 = 2 || UltimosProcesos van de (8 - (8 DIV 3) * 3 = 2) a (3 - 1 = 2).\n",
        "\n",
        "    Es decir, los últimos procesos van de 2 a 2 (solo el 2)y cada uno hará 2 iteraciones\n",
        "\n",
        "By: Jeremy Axl Lazo Mendoza\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vww7AO93S8QD"
      },
      "source": [
        "### Exercises\n",
        "- Run, using these numbers of processes, N: 1, 2, 4, and 8\n",
        "- Compare source code to output.\n",
        "- Change REPS to 16, save, rerun, varying N again.\n",
        "- Explain how this pattern divides the iterations of the loop among the processes.\n",
        "\n",
        "Which of the following is the correct assignment of loop iterations to processes for this code, when REPS is 8 and numProcesses is 4?\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1eUsjxYdWXWqThO_rdLO91HaLqBLAAh_S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGj_Em7xToeB"
      },
      "source": [
        "# Point to point communication: the message passing pattern\n",
        "\n",
        "The fundamental basis of coordination between independent processes is point-to-point communication between processes through the communication links in the MPI.COMM_WORLD. The form of communication is called message passing, where one process **sends** data to another one, who in turn must **receive** it from the sender. This is illustrated as follows:\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1WJcOXq6Dn5TKF9Lng8r18_y2b23tHFe8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXSiO5dGuJda"
      },
      "source": [
        "## Message Passing Pattern: Key Problem\n",
        "\n",
        "The following code represents a common error that many programmers have inadvertently placed in their code. The concept behind this program is that we wish to use communication between pairs of processes, like this:\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1UJ2acj6XzphD2W6gnutNF2YGp6wU529Z)\n",
        "\n",
        "For message passing to work between a pair of processes, one must send and the other must receive. If we wish to **exchange** data, then each process will need to perform both a send and a receive.\n",
        "The idea is that process 0 will send data to process 1, who will receive it from process 0. Process 1 will also send some data to process 0, who will receive it from process 1. Similarly, processes 2 and 3 will exchange messages: process 2 will send data to process 3, who will receive it from process 2. Process 3 will also send some data to process 2, who will receive it from process 3.\n",
        "\n",
        "If we have more processes, we still want to pair up processes together to exchange messages. The mechanism for doing this is to know your process id. If your id is odd (1, 3 in the above diagram), you will send and receive from your neighbor whose id is id - 1. If your id is even (0, 2), you will send and receive from your neighbor whose id is id + 1. This should work even if we add more than 4 processes, as long as the number of processes is divisible by 2.\n",
        "\n",
        "![warning sign](https://drive.google.com/uc?id=1SEqDBTBSKwNVXzn-zueWa7fBCm5b1_MB)\n",
        "**Warning** There is a problem with the following code called *deadlock*. This happens when every process is waiting on an action from another process. The program cannot complete. **To stop the program, choose the small square that appears after you choose to run the mpirun cell.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_-_ypqDvAM0",
        "outputId": "ba3a352e-71d4-4ab9-8b7c-7a2db44ec69a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 04messagePssingDeadlock.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "# function to return whether a number of a process is odd or even\n",
        "def odd(number):\n",
        "    if (number % 2) == 0:\n",
        "        return False\n",
        "    else :\n",
        "        return True\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    if numProcesses > 1 and not odd(numProcesses):\n",
        "        sendValue = id\n",
        "        if odd(id):\n",
        "            #odd processes receive from their paired 'neighbor', then send\n",
        "            comm.send(sendValue, dest=id-1)\n",
        "            receivedValue = comm.recv(source=id-1)\n",
        "            \n",
        "        else :\n",
        "            #even processes receive from their paired 'neighbor', then send\n",
        "            comm.send(sendValue, dest=id+1)\n",
        "            receivedValue = comm.recv(source=id+1)\n",
        "\n",
        "        print(\"Process {} of {} on {} computed {} and received {}\"\\\n",
        "        .format(id, numProcesses, myHostName, sendValue, receivedValue))\n",
        "\n",
        "    else :\n",
        "        if id == 0:\n",
        "            print(\"Please run this program with the number of processes \\\n",
        "positive and even\")\n",
        "\n",
        "########## Run the main function\n",
        "main()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting 04messagePssingDeadlock.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbbAxy7vvYwW",
        "outputId": "f47a0ddc-dfb1-4e4f-9ca5-a7004c57f575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "! mpirun --allow-run-as-root -np 6 python 04messagePssingDeadlock.py"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process 4 of 6 on 59784989e831 computed 4 and received 5\n",
            "Process 5 of 6 on 59784989e831 computed 5 and received 4\n",
            "Process 3 of 6 on 59784989e831 computed 3 and received 2\n",
            "Process 2 of 6 on 59784989e831 computed 2 and received 3\n",
            "Process 0 of 6 on 59784989e831 computed 0 and received 1\n",
            "Process 1 of 6 on 59784989e831 computed 1 and received 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MLayXixxraM"
      },
      "source": [
        "![warning sign](https://drive.google.com/uc?id=1SEqDBTBSKwNVXzn-zueWa7fBCm5b1_MB)Remember,**To stop the program, choose the small square that appears after you choose to run the mpirun cell.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49L8NojO6axL"
      },
      "source": [
        "#### What causes the deadlock?\n",
        "\n",
        "Each process, regardless of its id, will execute a receive request first. In this model, recv is a **blocking** function- it will not continue until it gets data from a send. So every process is blocked waiting to receive a message.\n",
        "\n",
        "#### Can you think of how to fix this problem?\n",
        "\n",
        "Since recv is a **blocking** function, we need to have some processes send first, while others correspondingly recv first from those who send first. This provides coordinated exchanges.\n",
        "\n",
        "Go to the next example to see the solution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxKI85d8LLll"
      },
      "source": [
        "## Message Passing Patterns: avoiding deadlock\n",
        "\n",
        "Let's look at a few more correct message passing examples.\n",
        "\n",
        "### Fix the Deadlock\n",
        "\n",
        "To fix deadlock of the previous example, we coordinate the communication between pairs of processes so that there is an ordering of sends and receives between them.\n",
        "\n",
        "![Important symbol](https://drive.google.com/uc?id=1AWRLAqeaqi7SG7PHyOVywZRuMDK9Z2_s)**Important:** The new code corrects deadlock with a simple change: odd process sends first, even process receives first. *This is the proper pattern for exchanging data between pairs of processes.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO8N49wuL0fR",
        "outputId": "0ad719e6-17c9-486d-c8c8-b02e9f0d8ec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 05messagePassing.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "# function to return whether a number of a process is odd or even\n",
        "def odd(number):\n",
        "    if (number % 2) == 0:\n",
        "        return False\n",
        "    else :\n",
        "        return True\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    if numProcesses > 1 and not odd(numProcesses):\n",
        "        sendValue = id\n",
        "        if odd(id):\n",
        "            #odd processes send to their paired 'neighbor', then receive from\n",
        "            comm.send(sendValue, dest=id-1)\n",
        "            receivedValue = comm.recv(source=id-1)\n",
        "        else :\n",
        "            #even processes receive from their paired 'neighbor', then send\n",
        "            receivedValue = comm.recv(source=id+1)\n",
        "            comm.send(sendValue, dest=id+1)\n",
        "\n",
        "        print(\"Process {} of {} on {} computed {} and received {}\"\\\n",
        "        .format(id, numProcesses, myHostName, sendValue, receivedValue))\n",
        "\n",
        "    else :\n",
        "        if id == 0:\n",
        "            print(\"Please run this program with the number of processes \\\n",
        "positive and even\")\n",
        "\n",
        "########## Run the main function\n",
        "main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting 05messagePassing.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zl-Ms_kMMql",
        "outputId": "0bafc2a3-192b-4e7f-baee-94ff0b6e89ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "! mpirun --allow-run-as-root -np 6 python 05messagePassing.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process 2 of 6 on 4afe557c9fa0 computed 2 and received 3\n",
            "Process 3 of 6 on 4afe557c9fa0 computed 3 and received 2\n",
            "Process 0 of 6 on 4afe557c9fa0 computed 0 and received 1\n",
            "Process 4 of 6 on 4afe557c9fa0 computed 4 and received 5\n",
            "Process 1 of 6 on 4afe557c9fa0 computed 1 and received 0\n",
            "Process 5 of 6 on 4afe557c9fa0 computed 5 and received 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKgBEDizMkRM"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "- Run, using N = 4, 6, 8, and 10 processes. (Note what happens if you use an odd number instead.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4ybJILhM137"
      },
      "source": [
        "## Sending data structures\n",
        "This next example illustrates that we can exchange different lists of data between processes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grQWl82lNWsn",
        "outputId": "68dae287-fc01-4019-fd62-92ca176a37c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 06messagePassing2.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "# function to return whether a number of a process is odd or even\n",
        "def odd(number):\n",
        "    if (number % 2) == 0:\n",
        "        return False\n",
        "    else :\n",
        "        return True\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    if numProcesses > 1 and not odd(numProcesses):\n",
        "        #generate a list of 8 numbers, beginning with my id\n",
        "        sendList = list(range(id, id+8))\n",
        "        if odd(id):\n",
        "            #odd processes send to their 'left neighbor', then receive from\n",
        "            comm.send(sendList, dest=id-1)\n",
        "            receivedList = comm.recv(source=id-1)\n",
        "        else :\n",
        "            #even processes receive from their 'right neighbor', then send\n",
        "            receivedList = comm.recv(source=id+1)\n",
        "            comm.send(sendList, dest=id+1)\n",
        "\n",
        "        print(\"Process {} of {} on {} computed {} and received {}\"\\\n",
        "        .format(id, numProcesses, myHostName, sendList, receivedList))\n",
        "\n",
        "    else :\n",
        "        if id == 0:\n",
        "            print(\"Please run this program with the number of processes \\\n",
        "positive and even\")\n",
        "\n",
        "########## Run the main function\n",
        "main()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting 06messagePassing2.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zYlFcz8Nnx1",
        "outputId": "0cfdfad0-f091-4425-a85e-11291b9b269f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "! mpirun --allow-run-as-root -np 4 python 06messagePassing2.py"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process 2 of 4 on 59784989e831 computed [2, 3, 4, 5, 6, 7, 8, 9] and received [3, 4, 5, 6, 7, 8, 9, 10]\n",
            "Process 3 of 4 on 59784989e831 computed [3, 4, 5, 6, 7, 8, 9, 10] and received [2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Process 0 of 4 on 59784989e831 computed [0, 1, 2, 3, 4, 5, 6, 7] and received [1, 2, 3, 4, 5, 6, 7, 8]\n",
            "Process 1 of 4 on 59784989e831 computed [1, 2, 3, 4, 5, 6, 7, 8] and received [0, 1, 2, 3, 4, 5, 6, 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFx0VyZzN1Aw"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "- Run, using N = 4, 6, 8, and 10 processes. \n",
        "- In the above code, locate where the list of elements to be sent is being made by each process. What is different about each list per process?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJQfgyqlOTCr"
      },
      "source": [
        "## Ring of passed messages\n",
        "Another pattern that appears in message passing programs is to use a ring of processes, where messages get sent in this fashion:\n",
        "\n",
        "![picture of ring of message passing](https://drive.google.com/uc?id=16VMF9t8nD3JcVehFvs4dbzIiU5eDuZbG)\n",
        "\n",
        "When we have 4 processes, the idea is that process 0 will send data to process 1, who will receive it from process 0 and then send it to process 2, who will receive it from process 1 and then send it to process 3, who will receive it from process 2 and then send it back around to process 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4PQNTd3PAGR",
        "outputId": "23b57272-279b-4f5e-ed28-3c9be9853fc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 07messagePassing3.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    if numProcesses > 1 :\n",
        "\n",
        "        if id == 0:        # master\n",
        "            #generate a list with master id in it\n",
        "            sendList = [id]\n",
        "            # send to the first worker\n",
        "            comm.send(sendList, dest=id+1)\n",
        "            print(\"Master Process {} of {} on {} sent {}\"\\\n",
        "            .format(id, numProcesses, myHostName, sendList))\n",
        "            # receive from the last worker\n",
        "            receivedList = comm.recv(source=numProcesses-1)\n",
        "            print(\"Master Process {} of {} on {} received {}\"\\\n",
        "            .format(id, numProcesses, myHostName, receivedList))\n",
        "        else :\n",
        "            # worker: receive from any source\n",
        "            receivedList = comm.recv(source=id-1)\n",
        "            # add this worker's id to the list and send along to next worker,\n",
        "            # or send to the master if the last worker\n",
        "            sendList = receivedList + [id]\n",
        "            comm.send(sendList, dest=(id+1) % numProcesses)\n",
        "\n",
        "            print(\"Worker Process {} of {} on {} received {} and sent {}\"\\\n",
        "            .format(id, numProcesses, myHostName, receivedList, sendList))\n",
        "\n",
        "    else :\n",
        "        print(\"Please run this program with the number of processes \\\n",
        "greater than 1\")\n",
        "\n",
        "########## Run the main function\n",
        "main()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing 07messagePassing3.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGTpIE-pPRIq",
        "outputId": "7425bca0-8c66-413a-a516-60ce0567641f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "! mpirun --allow-run-as-root -np 3 python 07messagePassing3.py"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Master Process 0 of 3 on 59784989e831 sent [0]\n",
            "Worker Process 2 of 3 on 59784989e831 received [0, 1] and sent [0, 1, 2]\n",
            "Worker Process 1 of 3 on 59784989e831 received [0] and sent [0, 1]\n",
            "Master Process 0 of 3 on 59784989e831 received [0, 1, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6bvSqgMPeyt"
      },
      "source": [
        "### Exercises\n",
        "- Run, using N = from 1 through 8 processes.\n",
        "- Make sure that you can trace how the code generates the output that you see.\n",
        "- How is the finishing of the 'ring' completed, where the last process determines that it should send back to process 0?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPqOxvbK5qSx"
      },
      "source": [
        "# Collective Communication: Broadcast pattern\n",
        "There are many cases when a master process obtains or creates data that needs to be sent to all of the other processes. There is a special pattern for this called **broadcast**. You will see examples of the master sending different types of data to each of the other processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57wzDFu2xxvW"
      },
      "source": [
        "## Broadcast from master to workers\n",
        "\n",
        "We will look at three types of data that can be created in the master and sent to the workers. Rather than use send and receive, we will use a special new function called bcast.\n",
        "\n",
        "![Important symbol](https://drive.google.com/uc?id=1AWRLAqeaqi7SG7PHyOVywZRuMDK9Z2_s) **Note:** In each code example, note how the master does one thing, and the workers do another, but **all of the processes execute the bcast function.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3eZ1sN1yVsK"
      },
      "source": [
        "### Broadcast a dictionary\n",
        "\n",
        "Find the place in this code where the data is being broadcast to all of the processes. Match the prints to the output you observe when you run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVx87ecmynYO",
        "outputId": "5ee4f839-4de5-417c-c57a-bd1828040e27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 08broadcast.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    if numProcesses > 1 :\n",
        "\n",
        "        if id == 0:        # master\n",
        "            #master: generate a dictionary with arbitrary data in it\n",
        "            data = {'one': 1, 'two': 2, 'three': 3}\n",
        "            print(\"Master Process {} of {} on {} broadcasts {}\"\\\n",
        "            .format(id, numProcesses, myHostName, data))\n",
        "\n",
        "        else :\n",
        "            # worker: start with empty data\n",
        "            data = {}\n",
        "            print(\"Worker Process {} of {} on {} starts with {}\"\\\n",
        "            .format(id, numProcesses, myHostName, data))\n",
        "\n",
        "        #initiate and complete the broadcast\n",
        "        data = comm.bcast(data, root=0)\n",
        "        #check the result\n",
        "        print(\"Process {} of {} on {} has {} after the broadcast\"\\\n",
        "        .format(id, numProcesses, myHostName, data))\n",
        "\n",
        "    else :\n",
        "        print(\"Please run this program with the number of processes \\\n",
        "greater than 1\")\n",
        "\n",
        "########## Run the main function\n",
        "main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing 08broadcast.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK268l7SzB5e",
        "outputId": "eac1dac2-7a00-4c6a-a10b-273db1b83b39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "source": [
        "! mpirun --allow-run-as-root -np 4 python 08broadcast.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Worker Process 2 of 4 on 4afe557c9fa0 starts with {}\n",
            "Worker Process 3 of 4 on 4afe557c9fa0 starts with {}\n",
            "Master Process 0 of 4 on 4afe557c9fa0 broadcasts {'one': 1, 'two': 2, 'three': 3}\n",
            "Worker Process 1 of 4 on 4afe557c9fa0 starts with {}\n",
            "Process 0 of 4 on 4afe557c9fa0 has {'one': 1, 'two': 2, 'three': 3} after the broadcast\n",
            "Process 1 of 4 on 4afe557c9fa0 has {'one': 1, 'two': 2, 'three': 3} after the broadcast\n",
            "Process 3 of 4 on 4afe557c9fa0 has {'one': 1, 'two': 2, 'three': 3} after the broadcast\n",
            "Process 2 of 4 on 4afe557c9fa0 has {'one': 1, 'two': 2, 'three': 3} after the broadcast\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNe1kQLpyqh2"
      },
      "source": [
        "#### Exercise\n",
        "- Run, using N = from 1 through 8 processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcckAX8G0Pd-"
      },
      "source": [
        "### Broadcast user input\n",
        "\n",
        "The following program will take extra input that will get broadcast to all processes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI7oPCn309N_",
        "outputId": "783c78e5-d08b-485b-c592-7ac4feb6714c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 09broadcastUserInput.py\n",
        "from mpi4py import MPI\n",
        "import sys\n",
        "\n",
        "# Determine if user provided a string to be broadcast.\n",
        "# If not, quit with a warning.\n",
        "def checkInput(id):\n",
        "    numArguments = len(sys.argv)\n",
        "    if numArguments == 1:\n",
        "        #no extra argument was given- master warns and all exit\n",
        "        if id == 0:\n",
        "            print(\"Please add a string to be broadcast from master to workers\")\n",
        "        sys.exit()\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    if numProcesses > 1 :\n",
        "        checkInput(id)\n",
        "\n",
        "        if id == 0:        # master\n",
        "            #master: get the command line argument\n",
        "            data = sys.argv[1]\n",
        "            print(\"Master Process {} of {} on {} broadcasts \\\"{}\\\"\"\\\n",
        "            .format(id, numProcesses, myHostName, data))\n",
        "\n",
        "        else :\n",
        "            # worker: start with empty data\n",
        "            data = 'No data'\n",
        "            print(\"Worker Process {} of {} on {} starts with \\\"{}\\\"\"\\\n",
        "            .format(id, numProcesses, myHostName, data))\n",
        "\n",
        "        #initiate and complete the broadcast\n",
        "        data = comm.bcast(data, root=0)\n",
        "        #check the result\n",
        "        print(\"Process {} of {} on {} has \\\"{}\\\" after the broadcast\"\\\n",
        "        .format(id, numProcesses, myHostName, data))\n",
        "\n",
        "    else :\n",
        "        print(\"Please run this program with the number of processes \\\n",
        "greater than 1\")\n",
        "\n",
        "########## Run the main function\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing 09broadcastUserInput.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUUqC5t11qJW"
      },
      "source": [
        "![warning sign](https://drive.google.com/uc?id=1SEqDBTBSKwNVXzn-zueWa7fBCm5b1_MB)\n",
        "**Warning** This program is unlike any of the others and takes in a second argument, as shown below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzeQF4rB1aa3",
        "outputId": "b71f21ca-177a-4982-c98f-7e1783d3489d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "source": [
        "! mpirun --allow-run-as-root -np 4 python 09broadcastUserInput.py \"hello world!\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Worker Process 1 of 4 on 4afe557c9fa0 starts with \"No data\"\n",
            "Worker Process 2 of 4 on 4afe557c9fa0 starts with \"No data\"\n",
            "Worker Process 3 of 4 on 4afe557c9fa0 starts with \"No data\"\n",
            "Master Process 0 of 4 on 4afe557c9fa0 broadcasts \"hello world!\"\n",
            "Process 0 of 4 on 4afe557c9fa0 has \"hello world!\" after the broadcast\n",
            "Process 1 of 4 on 4afe557c9fa0 has \"hello world!\" after the broadcast\n",
            "Process 2 of 4 on 4afe557c9fa0 has \"hello world!\" after the broadcast\n",
            "Process 3 of 4 on 4afe557c9fa0 has \"hello world!\" after the broadcast\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocfu_ZYw3QCz"
      },
      "source": [
        "#### Exercise\n",
        "- Run, using N = from 1 through 8 processes, with a string of your choosing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gcBP4q93eCj"
      },
      "source": [
        "### Broadcast a list\n",
        "\n",
        "This is just one more example to show that other data structures can also be broadcast from the master to all worker processes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8k_sECN3xCv",
        "outputId": "445caa29-8691-4c75-a217-5a5152e022fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile 11broadcastList.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    if numProcesses > 1 :\n",
        "\n",
        "        if id == 0:        # master\n",
        "            #master: generate a dictionary with arbitrary data in it\n",
        "            data = list(range(numProcesses))\n",
        "            print(\"Master Process {} of {} on {} broadcasts {}\"\\\n",
        "            .format(id, numProcesses, myHostName, data))\n",
        "\n",
        "        else :\n",
        "            # worker: start with empty data\n",
        "            data = []\n",
        "            print(\"Worker Process {} of {} on {} starts with {}\"\\\n",
        "            .format(id, numProcesses, myHostName, data))\n",
        "\n",
        "        #initiate and complete the broadcast\n",
        "        data = comm.bcast(data, root=0)\n",
        "\n",
        "        #check the result\n",
        "        print(\"Process {} of {} on {} has {} after the broadcast\"\\\n",
        "        .format(id, numProcesses, myHostName, data))\n",
        "\n",
        "    else :\n",
        "        print(\"Please run this program with the number of processes greater than 1\")\n",
        "\n",
        "########## Run the main function\n",
        "main()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing 11broadcastList.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSGPEJ244FdZ",
        "outputId": "053362ac-98bc-47e0-a3f9-ccf4871f7318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "source": [
        "! mpirun --allow-run-as-root -np 4 python 11broadcastList.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Worker Process 2 of 4 on 4afe557c9fa0 starts with []\n",
            "Worker Process 3 of 4 on 4afe557c9fa0 starts with []\n",
            "Master Process 0 of 4 on 4afe557c9fa0 broadcasts [0, 1, 2, 3]\n",
            "Process 0 of 4 on 4afe557c9fa0 has [0, 1, 2, 3] after the broadcast\n",
            "Worker Process 1 of 4 on 4afe557c9fa0 starts with []\n",
            "Process 1 of 4 on 4afe557c9fa0 has [0, 1, 2, 3] after the broadcast\n",
            "Process 2 of 4 on 4afe557c9fa0 has [0, 1, 2, 3] after the broadcast\n",
            "Process 3 of 4 on 4afe557c9fa0 has [0, 1, 2, 3] after the broadcast\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGWlHIPh4t08"
      },
      "source": [
        "#### Exercise\n",
        "- Run, using N = from 1 through 8 processes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogLec1ig6HZj"
      },
      "source": [
        "# Collective Communication: reduction pattern\n",
        "\n",
        "There are often cases when every process needs to complete a partial result of an overall computation. For example if you want to process a large set of numbers by summing them together into one value (i.e. *reduce* a set of numbers into one value, its sum), you could do this faster by having each process compute a partial sum, then have all the processes communicate to add each of their partial sums together.\n",
        "\n",
        "This is so common in parallel processing that there is a special collective communication function called **reduce** that does just this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwiw83d44_gR"
      },
      "source": [
        "## Collective Communication: reduce function\n",
        "\n",
        "The type of reduction of many values down to one can be done with different types of operators on the set of values computed by each process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrwM90WO6jKv"
      },
      "source": [
        "### Reduce all values using sum and max\n",
        "In this example, every process computes the square of (id+1). Then all those values are summed together and also the maximum function is applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jPJPMgE5U_t"
      },
      "source": [
        "%%writefile 12reduction.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    square = (id+1) * (id+1)\n",
        "\n",
        "    if numProcesses > 1 :\n",
        "        #initiate and complete the reductions\n",
        "        sum = comm.reduce(square, op=MPI.SUM)\n",
        "        max = comm.reduce(square, op=MPI.MAX)\n",
        "    else :\n",
        "        sum = square\n",
        "        max = square\n",
        "\n",
        "    if id == 0:        # master/root process will print result\n",
        "        print(\"The sum of the squares is  {}\".format(sum))\n",
        "        print(\"The max of the squares is  {}\".format(max))\n",
        "\n",
        "########## Run the main function\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xAazDP17GJJ"
      },
      "source": [
        "! mpirun --allow-run-as-root -np 4 python 12reduction.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRLLMm9j7iOA"
      },
      "source": [
        "#### Exercises\n",
        "- Run, using N = from 1 through 8 processes.\n",
        "- Try replacing MPI.MAX with MPI.MIN(minimum) and/or replacing MPI.SUM with MPI.PROD (product). Then save and run the code again.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnO-S8V37tpD"
      },
      "source": [
        "### Reduction on a list of values\n",
        "\n",
        "We can try reduction with lists of values, but the behavior matches Python semantics regarding lists. Note this in the following example. Then note how you can change the semantics in the exercises.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsAow5Ds8DAv"
      },
      "source": [
        "%%writefile 13reductionList.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "# Exercise: Can you explain what this function returns,\n",
        "#           given two lists as input?\n",
        "def sumListByElements(x,y):\n",
        "    return [a+b for a, b in zip(x, y)]\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    srcList = [1*id, 2*id, 3*id, 4*id, 5*id]\n",
        "\n",
        "    destListMax = comm.reduce(srcList, op=MPI.MAX)\n",
        "    destListSum = comm.reduce(srcList, op=MPI.SUM)\n",
        "    #destListSumByElement = comm.reduce(srcList, op=sumListByElements)\n",
        "\n",
        "    if id == 0:        # master/root process will print result\n",
        "        print(\"The resulting reduce max list is  {}\".format(destListMax))\n",
        "        print(\"The resulting reduce sum list is  {}\".format(destListSum))\n",
        "        #print(\"The resulting reduce sum list is  {}\".format(destListSumByElement))\n",
        "\n",
        "########## Run the main function\n",
        "main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uow-rYeS8rxc"
      },
      "source": [
        "! mpirun --allow-run-as-root -np 4 python 13reductionList.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P74yRNds9SBx"
      },
      "source": [
        "#### Exercises\n",
        "- Run, using N = from 1 through 4 processes.\n",
        "- Uncomment the two lines of runnable code that are commented in the main() function. Observe the new results and explain why the MPI.SUM (using the + operator underneath) behaves the way it does on lists, and what the new function called sumListByElements is doing instead.\n",
        "\n",
        "![Important symbol](https://drive.google.com/uc?id=1AWRLAqeaqi7SG7PHyOVywZRuMDK9Z2_s) **Note:** There are two ways in Python that you might want to sum a set of lists from each process: 1) concatenating the elements together, or 2) summing the element at each location from each process and placing the sum in that location in a new list. In the latter case, the new list is the same length as the original lists on each process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYFc1mVp-e2r"
      },
      "source": [
        "# Collective Communication: scatter and gather pattern\n",
        "\n",
        "There are often cases when each process can work on some portion of a larger data structure. This can be carried out by having the master process maintain the larger structure and send parts to each of the worker processes, keeping part of the structure on the master. Each process then works on their portion of the data, and then the master can get the completed portions back.\n",
        "\n",
        "This is so common in message passing parallel processing that there are two special collective communication functions called **scatter** and **gather** that handle this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL2CQgIo-xTd"
      },
      "source": [
        "## Collective Communication: scatter and gather lists\n",
        "\n",
        "When several processes need to work on portions of a data structure, such as a list of lists or a 1-d or 2-d array, at various points in a program, a way to do this is to have one node, usually the master, divide the data structure and send portions to each of the other processes, often keeping one portion for itself. Each process then works on that portion of the data, and then the master can get the completed portions back. This type of coordination is so common that MPI has special patterns for it called **scatter** and **gather**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g5HzFU1--zY"
      },
      "source": [
        "### Scatter Lists\n",
        "The following diagrams illustrate how scatter using python list structures works. The master contains a list of lists and all processes participate in the scatter:\n",
        "\n",
        "![scatter lists diagram](https://drive.google.com/uc?id=1QDRW2JeAa_TelKxZTphCPF393Bxn_BbL)\n",
        "\n",
        "After the scatter is completed, each process has one of the smaller lists to work on, like this:\n",
        "\n",
        "![after scatter lists diagram](https://drive.google.com/uc?id=1xA2NRtm1k4_g16tJTWBFCVArKLfIEurc)\n",
        "\n",
        "In this next code example, some small lists are created in a list whose length is as long as the number of processes.\n",
        "\n",
        "![Important symbol](https://drive.google.com/uc?id=1AWRLAqeaqi7SG7PHyOVywZRuMDK9Z2_s) **Note:** In the code below, note how all processes must call the scatter function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI6C7UH7AiMV"
      },
      "source": [
        "%%writefile 14scatter.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "# Create a list of lists to be scattered.\n",
        "def genListOfLists(numElements):\n",
        "    data = [[0]*3 for i in range(numElements)]\n",
        "    for i in range(numElements):\n",
        "        #make small lists of 3 distinct elements\n",
        "        smallerList = []\n",
        "        for j in range(1,4):\n",
        "            smallerList = smallerList + [(i+1)*j]\n",
        "        # place the small list in the larger list\n",
        "        data[i] = smallerList\n",
        "    return data\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    # in mpi4py, the lowercase scatter method only works on lists whose size\n",
        "    # is the total number of processes.\n",
        "    numElements = numProcesses      #total elements in list created by master process\n",
        "\n",
        "    # however, the list can contain lists, like this list of 3-element lists,\n",
        "    # for example this list of four 3-element lists:\n",
        "    #     [[1, 2, 3], [2, 4, 6], [3, 6, 9], [4, 8, 12]]\n",
        "\n",
        "    if id == 0:\n",
        "        data = genListOfLists(numElements)\n",
        "        print(\"Master {} of {} on {} has created list: {}\"\\\n",
        "        .format(id, numProcesses, myHostName, data))\n",
        "    else:\n",
        "        data = None\n",
        "        print(\"Worker Process {} of {} on {} starts with {}\"\\\n",
        "        .format(id, numProcesses, myHostName, data))\n",
        "\n",
        "    #scatter one small list in the large list on node 0 to each of the processes\n",
        "    result = comm.scatter(data, root=0)\n",
        "\n",
        "    print(\"Process {} of {} on {} has result after scatter {}\"\\\n",
        "    .format(id, numProcesses, myHostName, result))\n",
        "\n",
        "    if id == 0:\n",
        "        print(\"Master {} of {} on {} has original list after scatter: {}\"\\\n",
        "        .format(id, numProcesses, myHostName, data))\n",
        "\n",
        "########## Run the main function\n",
        "main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgt8fHPaA0CH"
      },
      "source": [
        "! mpirun --allow-run-as-root -np 4 python 14scatter.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9EYqJldBRS-"
      },
      "source": [
        "#### Exercises\n",
        "- Run, using N = from 2 through 8 processes.\n",
        "- If you want to study the code, explain to yourself what genListofLists does in the code below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIw9YI6GB5go"
      },
      "source": [
        "### Gather Lists\n",
        "Once several processes have their own lists of data, those lists can also be gathered back together into a list of lists, usually in the master process. All processes participate in a gather, like this:\n",
        "\n",
        "![before gather diagram](https://drive.google.com/uc?id=1OWHNMKCEKsGpExJCO6l5czW9QFyMZiT6)\n",
        "\n",
        "The gather creates a list of lists in the master, like this:\n",
        "\n",
        "![after gather diagram](https://drive.google.com/uc?id=1W9lky1LY0L0K6iyA00jsNV4hAnmmbvP2)\n",
        "\n",
        "In this example, each process creates some very small lists. Then a gather is used to create a list of lists on the master process.\n",
        "\n",
        "![Important symbol](https://drive.google.com/uc?id=1AWRLAqeaqi7SG7PHyOVywZRuMDK9Z2_s) **Note:** In the code below, note how all processes must call the gather function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPk2IBX_C46Z"
      },
      "source": [
        "%%writefile 15gather.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "SMALL_LIST_SIZE = 3\n",
        "\n",
        "# create a small list whose values contain id times multiples of 10\n",
        "def genSmallList(id):\n",
        "    smallerList = []\n",
        "    for j in range(1, SMALL_LIST_SIZE+1):\n",
        "        smallerList = smallerList + [(id * 10)*j]\n",
        "    return smallerList\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    #all processes create small lists\n",
        "    sendData = genSmallList(id)\n",
        "    print(\"Process {} of {} on {} starts with {}\"\\\n",
        "    .format(id, numProcesses, myHostName, sendData))\n",
        "\n",
        "    # gather the small lists at the master node:\n",
        "    # final result is a list whose length == the number of processes\n",
        "    result = comm.gather(sendData, root=0)\n",
        "\n",
        "    # only the master node has all of the small lists\n",
        "    if id == 0:\n",
        "        print(\"Process {} of {} on {} has result after gather {}\"\\\n",
        "        .format(id, numProcesses, myHostName, result))\n",
        "\n",
        "########## Run the main function\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvdQljANDOtE"
      },
      "source": [
        "! mpirun --allow-run-as-root -np 4 python 15gather.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPVwi5W4DpwJ"
      },
      "source": [
        "#### Exercises\n",
        "- Run, using N = from 2 through 8 processes.\n",
        "- Try with different values of SMALL_LIST_SIZE, perhaps changing printing of result for readability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eCGJi2hEOC1"
      },
      "source": [
        "## Collective Communication:  scatter and gather arrays\n",
        "\n",
        "The mpi4py library of functions has several collective communication functions that are designed to work with arrays created using the python library for numerical analysis computations called *numpy*.\n",
        "\n",
        "If you are unfamiliar with using numpy, and want to know more about its features and available methods, you will need to consult another tutorial for that. It should be possible to understand the following scatter, then gather example by observing the results that get printed, even if you are unfamiliar with the functions from numpy that are used to create the 1-D array.\n",
        "\n",
        "The numpy library has special data structures called arrays, that are common in other programming languages. A 1-dimensional array of integers can be envisioned very much like a list of integers, where each value in the array is at a particular index. The mpi4py Scatter function, with a capital S, can be used to send portions of a larger array on the master to the workers, like this:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1n2YmY12tBrTxtJK6MFpBX9nWmopQGT_s)\n",
        "\n",
        "The result of doing this then looks like this, where each process has a portion of the original that they can then work on:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=19GNbTWWJEOU16wNjwzHon4jpC5fXKj_1)\n",
        "\n",
        "The reverse of this process can be done using the Gather function.\n",
        "\n",
        "In this example, a 1-D array is created by the master, then scattered, using Scatter (capital S). After each smaller array used by each process is changed, the Gather (capital G) function brings the full array with the changes back into the master.\n",
        "\n",
        "![Important symbol](https://drive.google.com/uc?id=1AWRLAqeaqi7SG7PHyOVywZRuMDK9Z2_s) **Note:** In the code below, note how all processes must call the Scatter and Gather functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO06o4HAFBZR"
      },
      "source": [
        "%%writefile 16ScatterGather.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Create a 1D array to be scattered.\n",
        "def genArray(numProcesses, numElementsPerProcess):\n",
        "\n",
        "    data = np.linspace(1, #start\n",
        "                numProcesses*numElementsPerProcess, #stop\n",
        "                numElementsPerProcess*numProcesses, #total elements\n",
        "                dtype='u4')  # 4-byte unsigned integer data type\n",
        "    return data\n",
        "\n",
        "def timesTen(a):\n",
        "    return(a*10);\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    # in mpi4py, the uppercase Scatter method works on arrays generated by\n",
        "    # numpy routines.\n",
        "    #\n",
        "    # Here we will create a single array designed to then scatter 3 elements\n",
        "    # of it in a smaller array to each process.\n",
        "\n",
        "    numDataPerProcess = 3\n",
        "\n",
        "    if id == 0:\n",
        "        data = genArray(numProcesses, numDataPerProcess)\n",
        "        #genListOfLists(numElements)\n",
        "        print(\"Master {} of {} on {} has created array: {}\"\\\n",
        "        .format(id, numProcesses, myHostName, data))\n",
        "    else:\n",
        "        data = None\n",
        "        print(\"Worker Process {} of {} on {} starts with {}\"\\\n",
        "        .format(id, numProcesses, myHostName, data))\n",
        "\n",
        "    #scatter one small array from a part of the large array\n",
        "    # on node 0 to each of the processes\n",
        "    smallerPart = np.empty(numDataPerProcess, dtype='u4') # allocate space for result on each process\n",
        "    comm.Scatter(data, smallerPart, root=0)\n",
        "\n",
        "    if id == 0:\n",
        "        print(\"Master {} of {} on {} has original array after Scatter: {}\"\\\n",
        "        .format(id, numProcesses, myHostName, data))\n",
        "\n",
        "    print(\"Process {} of {} on {} has smaller part after Scatter {}\"\\\n",
        "    .format(id, numProcesses, myHostName, smallerPart))\n",
        "\n",
        "    # do some work on each element\n",
        "    newValues = timesTen(smallerPart)\n",
        "\n",
        "    print(\"Process {} of {} on {} has smaller part after work {}\"\\\n",
        "    .format(id, numProcesses, myHostName, newValues))\n",
        "\n",
        "    # All processes participate in gathering each of their parts back at\n",
        "    # process 0, where the original data is now overwritten with new values\n",
        "    # from eqch process.\n",
        "    comm.Gather(newValues, data, root=0)\n",
        "\n",
        "    if id == 0:\n",
        "        print(\"Master {} of {} on {} has new data array after Gather:\\n {}\"\\\n",
        "        .format(id, numProcesses, myHostName, data))\n",
        "\n",
        "\n",
        "########## Run the main function\n",
        "main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMIeHGkDFQ1V"
      },
      "source": [
        "! mpirun --allow-run-as-root -np 4 python 16ScatterGather.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKhQXI7oFRMf"
      },
      "source": [
        "#### Exercises\n",
        "- Run, using N = from 2 through 8 processes.\n",
        "- If you want to study the numpy part of the code, look up the numpy function linspace used in genArray().\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnjHsPScGpEP"
      },
      "source": [
        "# When amount of work varies: balancing the load\n",
        "\n",
        "There are algorithms where the master is used to assign tasks to workers by sending them data and receiving results back as each worker completes a task (or after the worker completes all of its tasks). In many of these cases, the computation time needed by each worker process for each of its tasks can vary somewhat dramatically. This situation is where **dynamic load balancing** can be helpful.\n",
        "\n",
        "In this example we combine the master-worker pattern with message passing. The master has many tasks that need to be completed. The master starts by sending some data needed to complete a task to each worker process. Then the master loops and waits to hear back from each worker by receiving a message from any of them. When the master receives a message from a worker, it sends that worker more data for its next task, unless there are no more tasks to complete, in which case it sends a special message to the worker to stop running.\n",
        "\n",
        "In this simple example, each worker is sent the number of seconds it should 'sleep', which can vary from 1 to 8. This illustrates varying sizes of workloads. Because of the code's simplicity, the number of tasks each worker does doesn't vary by much. In some real examples, the time for one task my be quite different than the time for another, which could have a different outcome, in which some workers were able to complete more tasks as others were doing long ones.\n",
        "\n",
        "This approach can sometimes be an improvement on the assignment of an equal number of tasks to all processes.\n",
        "\n",
        "Note in this case how the master, whose id is 0, handles the assignment of tasks, while the workers simply do what they are sent until they are told to stop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow-7CfeCHY3n"
      },
      "source": [
        "%%writefile 17dynamicLoadBalance.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def genTasks(numTasks):\n",
        "    np.random.seed(1000)  # run the same set of timed tasks\n",
        "    return np.random.randint(low=1, high=9, size=numTasks)\n",
        "\n",
        "# tags that can be applied to messages\n",
        "WORKTAG = 1\n",
        "DIETAG = 2\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    if (id == 0) :\n",
        "        # create an arbitrary array of numbers for how long each\n",
        "        # worker task will 'work', by sleeping that amount of seconds\n",
        "        numTasks = (numProcesses-1)*4 # avg 4 tasks per worker process\n",
        "        workTimes = genTasks(numTasks)\n",
        "        print(\"master created {} values for sleep times:\".format(workTimes.size), flush=True)\n",
        "        print(workTimes, flush=True)\n",
        "        handOutWork(workTimes, comm, numProcesses)\n",
        "    else:\n",
        "        worker(comm)\n",
        "\n",
        "def handOutWork(workTimes, comm, numProcesses):\n",
        "    totalWork = workTimes.size\n",
        "    workcount = 0\n",
        "    recvcount = 0\n",
        "    print(\"master sending first tasks\", flush=True)\n",
        "    # send out the first tasks to all workers\n",
        "    for id in range(1, numProcesses):\n",
        "        if workcount < totalWork:\n",
        "            work=workTimes[workcount]\n",
        "            comm.send(work, dest=id, tag=WORKTAG)\n",
        "            workcount += 1\n",
        "            print(\"master sent {} to {}\".format(work, id), flush=True)\n",
        "\n",
        "    # while there is still work,\n",
        "    # receive result from a worker, which also\n",
        "    # signals they would like some new work\n",
        "    while (workcount < totalWork) :\n",
        "        # receive next finished result\n",
        "        stat = MPI.Status()\n",
        "        workTime = comm.recv(source=MPI.ANY_SOURCE, status=stat)\n",
        "        recvcount += 1\n",
        "        workerId = stat.Get_source()\n",
        "        print(\"master received {} from {}\".format(workTime, workerId), flush=True)\n",
        "        #send next work\n",
        "        comm.send(workTimes[workcount], dest=workerId, tag=WORKTAG)\n",
        "        workcount += 1\n",
        "        print(\"master sent {} to {}\".format(work, workerId), flush=True)\n",
        "\n",
        "    # Receive results for outstanding work requests.\n",
        "    while (recvcount < totalWork):\n",
        "        stat = MPI.Status()\n",
        "        workTime = comm.recv(source=MPI.ANY_SOURCE, status=stat)\n",
        "        recvcount += 1\n",
        "        workerId = stat.Get_source()\n",
        "        print(\"end: master received {} from {}\".format(workTime, workerId), flush=True)\n",
        "\n",
        "    # Tell all workers to stop\n",
        "    for id in range(1, numProcesses):\n",
        "        comm.send(-1, dest=id, tag=DIETAG)\n",
        "\n",
        "\n",
        "def worker(comm):\n",
        "    # keep receiving messages and do work, unless tagged to 'die'\n",
        "    while(True):\n",
        "        stat = MPI.Status()\n",
        "        waitTime = comm.recv(source=0, tag=MPI.ANY_TAG, status=stat)\n",
        "        print(\"worker {} got {}\".format(comm.Get_rank(), waitTime), flush=True)\n",
        "        if (stat.Get_tag() == DIETAG):\n",
        "            print(\"worker {} dying\".format(comm.Get_rank()), flush=True)\n",
        "            return\n",
        "        # simulate work by sleeping\n",
        "        time.sleep(waitTime)\n",
        "        # indicate done with work by sending to Master\n",
        "        comm.send(waitTime, dest=0)\n",
        "\n",
        "########## Run the main function\n",
        "main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t30ynerHaaN"
      },
      "source": [
        "! mpirun --allow-run-as-root -np 4 python 17dynamicLoadBalance.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t65YHXekHbBK"
      },
      "source": [
        "## Exercises\n",
        "- Run, using N = 4 processes\n",
        "- Study the execution carefully. Note that with 4 processes, 3 are workers. The total number of tasks is 3*4, or 12. Which process does the most work? You can count by looking for the lines that end with \"... from X\", where X is a worker process id.\n",
        "- Try with N = 8 (7 workers)."
      ]
    }
  ]
}